Covariance, correlation - t-test
x<-c(0,2,4,5,7,8,10)
y<-c(4,3,1,1,2,1,0)
mean(x)
mean(y)
sd(x)
sd(y)
var(x)
var(y)
cov(x,y)
cor(x,y)

x<-c(0,2,4,6,8)
y<-c(0,1,2,3,4)
mean(x)
mean(y)
sd(x)
sd(y)
var(x)
var(y)
cov(x,y)
cor(x,y)

Thus our correlation coefficient is 1. This means that the data = ”fully” linearly correlated, or that there is a function y = ax + b that translated the x data into the y data.

x<-c(77,65,73,58,63,49,51,82,103,69)
y<-c(102,73,56,55,83,72,88,70,81,85,44,71,62,78,75)
mean(x)
mean(y)
sd(x)
sd(y)
var(x)
var(y)
cov(x,y)
cor(x,y)
t.test(x, y, alternative = "two.sided", var.equal = FALSE)

Obs.:We reject the null hypothesis for too small values of T. Then, P(T(23) ≤ tα; H0) =
P(T(23) ≥ −tα; H0) = 0.10. By looking in the table we find −tα = 1.32, thus tα = −1.32.
Our t-value is not smaller than tα, we cannot reject the null hypothesis. Therefore we cannot
prove that people of age 40 are on average heavier than people of age 30.

x<-c(2,3,1,4,6,2,12,8)
y<-c(3,2,0,4,7,4,10,10)
mean(x)
mean(y)
var(x)
var(y)
sd(x)
sd(y)
cov(x,y)
cor(x,y)
t.test(x, y, paired = TRUE, alternative = "two.sided")
We reject our null hypothesis for too small values of T. Then, P(T(7) ≤ tα; H0) = P(T(7) ≥
−tα) = 0.05. By looking in the table we find −tα = 1.89, so tα = −1.89. Since our t-value
is not smaller than tα we cannot reject the null hypothesis. Therefore, we cannot prove that
the marketing strategy actually worked.

z<- c(206,	155,	246,	255,	196,	366,	270,	172,	262,	262, 224,	263,	296,	325,	142,	202,	322,	250,	456,	330, 186,	112,	115,	127,	266,	266,	415,	316,325, 292, 217,	235,	  72,	177,	281,	210,	192,	202, 49,	316, 261,	352,	 56,	311,	346, 78,	441,	364,	211,	231)

# Create the function.
getmode <- function(z) {
 uniqv <- unique(z)
uniqv[which.max(tabulate(match(z, uniqv)))]
result <- getmode(z)
print(result)

Mode = function(x){ 
     ta = table(x)
     tam = max(ta)
     if (all(ta == tam))
         mod = NA
     else
         if(is.numeric(x))
             mod = as.numeric(names(ta)[ta == tam])
    else
         mod = names(ta)[ta == tam]
     return(mod) }
Mode(z)

An experiment was conducted to compare the performance of two varieties of wheat, A and B. Seven farms were randomly chosen for the experiment, and the yields (in tonnes per hectare) for each variety on each farm were as follows:

Farm Number	1	2	3	4	5	6	7
Yield of Variety A	4.6	4.8	3.2	4.7	4.3	3.7	4.1
Yield of Variety B	4.1	4.0	3.5	4.1	4.5	3.2	3.8

a) 	Why do you think both varieties were tested on each farm, rather than testing Variety A on seven farms and variety B on seven other farms?

b)  	Assuming normal distributions, carry out a hypothesis test at 5% significance to test whether the mean yields are the same for the two varieties.

x<-c(4.6,	4.8,	3.2, 4.7,	4.3,	3.7,	4.1)
y<-c(4.1,	4.0,	3.5,	4.1,	4.5,	3.2,	3.8)

mean(x)
mean(y)
var(x)
var(y)
sd(x)
sd(y)
cov(x,y)
cor(x,y)
t.test(x, y, paired = TRUE, alternative = "two.sided")

7.16	C.V. = 2.45		T.S. = 2.01		H0 not rejected, yields could be same


 	    Cat	  Dog  TOTAL
Men	  207	  282   489
Women	231	  242   962
      438   524

A = matrix( 
          c(207, 282, 231, 242), # the data elements 
            nrow=2,              # number of rows 
            ncol=2,              # number of columns 
             byrow = TRUE)        # fill matrix by rows 
chisq.test(A) 

A = matrix( 
         c(106, 119, 25, 117, 141, 92), # the data elements 
         nrow=2,              # number of rows 
         ncol=3,              # number of columns 
          byrow = TRUE)        # fill matrix by rows 
chisq.test(A) 

A = matrix( 
                    c(5, 8, 7, 4, 7, 9, 7, 2, 3, 8, 5, 1, 4, 3, 6, 9, 3, 4, 10, 6, 2), # the data elements 
                   nrow=7,              # number of rows 
                   ncol=3,              # number of columns 
              byrow = TRUE)        # fill matrix by row
chisq.test(A)

#Multiple linear regression: Muitas vezes uma única variável preditora não será capaz de explicar tudo a respeito da variável resposta. Por exemplo, a renda de uma determinada pessoa (variável resposta) é influenciada por diversas variáveis, tais como sexo, idade, escolaridade entre outras. Assim, precisamos realizar uma regressão linear múltipla. É importante conhecer a fundo essa técnica, pois se o número de preditores (variáveis independentes) for grande, será necessário utilizar técnicas de subconjuntos visando remover preditores que não estejam associados às respostas, ajustando o modelo de regressão.

#Calculate the parameter estimates (β0, β1, β2, and σ2), in addition find the usual 95% confidence intervals for β0, β1, and β2.
You can copy the following lines to R to load the data:

D <- data.frame(
x1=c(0.58, 0.86, 0.29, 0.20, 0.56, 0.28, 0.08, 0.41, 0.22, 0.35,
0.59, 0.22, 0.26, 0.12, 0.65, 0.70, 0.30, 0.70, 0.39, 0.72,
0.45, 0.81, 0.04, 0.20, 0.95),
x2=c(0.71, 0.13, 0.79, 0.20, 0.56, 0.92, 0.01, 0.60, 0.70, 0.73,
0.13, 0.96, 0.27, 0.21, 0.88, 0.30, 0.15, 0.09, 0.17, 0.25,
0.30, 0.32, 0.82, 0.98, 0.00),
y=c(1.45, 1.93, 0.81, 0.61, 1.55, 0.95, 0.45, 1.14, 0.74, 0.98,
1.41, 0.81, 0.89, 0.68, 1.39, 1.53, 0.91, 1.49, 1.38, 1.73,
1.11, 1.68, 0.66, 0.69, 1.98)
)

# Multiple Linear Regression Example 
fit <- lm(y ~ x1 + x2, data=D)
summary(fit) # show results

# The error variance estimate is σ2 = 0.112. The confidence intervals can either be
calculated using the second column of the coefficient matrix, and the value of t0.975
(with degrees of freedom equal 22), or directly in R:
confint(fit, level=0.95) # CIs for model parameters 

# Other useful functions 
coefficients(fit) # model coefficients
fitted(fit) # predicted values
residuals(fit) # residuals
anova(fit) # anova table 
vcov(fit) # covariance matrix for model parameters 
influence(fit) # regression diagnostics

# diagnostic plots 
layout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page 

#Since the confidence interval for β2 cover zero (and the p-value is much larger than 0.05), the parameter should be removed from the model to get the simpler model the parameter estimates in the simpler model are and both parameters are now significant.
fit <- lm(y ~ x1, data=D)
summary(fit)

#Carry out a residual analysis to check that the model assumptions are fulfilled.
#We are interested in inspecting a q-q plot of the residuals and a plot of the residuals as a function of the fitted values. There are no strong evidence against the assumptions, the qq-plot is are a straight line and there are no obvious dependence between the residuals and the fitted values, and we conclude that the assumptions are fulfilled.

par(mfrow=c(1,2))
qqnorm(fit$residuals, pch=19)
qqline(fit$residuals)
plot(fit$fitted.values, fit$residuals, pch=19,
xlab="Fitted.values", ylab="Residuals")

#Make a plot of the fitted line and 95% confidence and prediction intervals of the line for x1 ∈ [0, 1] (it is assumed that the model was reduced above)
x1new <- seq(0,1,by=0.01)
pred <- predict(fit, newdata=data.frame(x1=x1new),
interval="prediction")
conf <- predict(fit, newdata=data.frame(x1=x1new),
interval="confidence")
plot(x1new, pred[ ,"fit"], type="l", ylim=c(0.1,2.4),
xlab="x1", ylab="Prediction")
lines(x1new, conf[ ,"lwr"], col="green", lty=2)
lines(x1new, conf[ ,"upr"], col="green", lty=2)
lines(x1new, pred[ ,"lwr"], col="red", lty=2)
lines(x1new, pred[ ,"upr"], col="red", lty=2)
legend("topleft", c("Prediction","Confidence band","Prediction band"),
lty=c(1,2,2), col=c(1,3,2), cex=0.7)

D <- data.frame(
y=c(9.29,12.67,12.42,0.38,20.77,9.52,2.38,7.46),
x1=c(1.00,2.00,3.00,4.00,5.00,6.00,7.00,8.00),
x2=c(4.00,12.00,16.00,8.00,32.00,24.00,20.00,28.00)
)

#The data is plotted with
par(mfrow=c(1,2))
plot(D$x1, D$y, xlab="x1", ylab="y")
plot(D$x2, D$y, xlab="x1", ylab="y")

#Estimate the parameters for the two models
Yi = β0 + β1x1,i + εi, εi ∼ N(0, σ2), and Yi = β0 + β1x2,i + εi, εi ∼ N(0, σ2), and report the 95% confidence intervals for the parameters. Are any of the parameters significantly different from zero on a 5% confidence level?

#The models are fitted with
fit1 <- lm(y ~ x1, data=D)
fit2 <- lm(y ~ x2, data=D)
confint(fit1)
confint(fit2)

#since all confidence intervals cover zero we cannot reject that the parameters are in fact zero, and we would conclude neither x1 nor x2 explain the variations in y

#Estimate the parameters for the model
Yi = β0 + β1x1,i + β2x2,i + εi, εi ∼ (N(0, σ2), (6-8) and go through the steps of Method 6.16 (use confidence level 0.05 in all tests).
#The model is fitted with
fit <- lm(y ~ x1 + x2, data=D)
summary(fit)

#Before discussing the parameter let’s have a look at the residuals:
par(mfrow=c(1,2))
qqnorm(fit$residuals)
qqline(fit$residuals)
plot(fit$fitted.values, fit$residuals,
xlab="Fitted values", ylab="Residuals")

#The are no obvious structures in the residuals as a function of the fitted values and also there does not seem be be serious departure from normality, but lets try to look at the residuals as a function of the independent variables anyway.

par(mfrow=c(1,2))
plot(D$x1, fit$residuals, xlab="x1", ylab="Residuals")
plot(D$x2, fit$residuals, xlab="x1", ylab="Residuals")

#the plot of the residuals as a function of x1 suggest that there could be a quadratic dependence.

#Now include the quadratic dependence of x1
D$x3 <- D$x1^2
fit3 <- lm(y ~ x1 + x2 + x3, data=D)
summary(fit3)

#we can see that all parameters are still significant, and we can do the residual analysis of the resulting model.

par(mfrow=c(2,2))
qqnorm(fit3$residuals)
qqline(fit3$residuals)
plot(fitted.values(fit3), fit3$residuals,
xlab="Fitted values", ylab="Residuals")

#The are no obvious structures in the residuals as a function of the fitted values and also there does not seem be be serious departure from normality, but lets try to look at the residuals as a function of the independent variables anyway

plot(D$x1, fit3$residuals, xlab="x1", ylab="Residuals")
plot(D$x2, fit3$residuals, xlab="x2", ylab="Residuals")

#Find the standard error for the line, and the confidence and prediction intervals for the line for the points (min(x1), min(x2)), (x¯1, x¯2), (max(x1), max(x2)).

## New data
Dnew <- data.frame(x1=c(min(D$x1),mean(D$x1),max(D$x1)),
x2=c(min(D$x2),mean(D$x2),max(D$x2)),
x3=c(min(D$x1),mean(D$x1),max(D$x1))^2)

## standard error for the line
predict(fit3, newdata=Dnew, se=TRUE)$se

## Confidence interval
predict(fit3, newdata=Dnew, interval="confidence")

## Confidence interval
predict(fit3, newdata=Dnew, interval="confidence")

#Plot the observed values together with the fitted values (e.g. as a functionof x1).
points(D$x1, fit3$fitted.values, pch="+", cex=2)
legend("topright", c("y1","fitted.values"), pch=c(19,3), col=c(2,1))

#Notice that we have an almost perfect fit when including x1, x2 and x1^2 in the model,  while neither x1 nor x2 alone could predict the outcomes.

#Exercise 1
#a. Load the state datasets.
data(state)

#b. Convert the state.x77 dataset to a dataframe.
state77 <- as.data.frame(state.x77)

#c. Rename the Life Exp variable to Life.Exp, and HS Grad to HS.Grad. (This avoids problems with referring to these variables when specifying a model.)
names(state77)[4] <- "Life.Exp"
names(state77)[6] <- "HS.Grad"

#Exercise 2: Suppose we wanted to enter all the variables in a first-order linear regression model with Life Expectancy as the dependent variable. Fit this model.
model <- lm(Life.Exp ~ ., data=state77)
#the '.' means 'all'
summary(model)

#Exercise 3: Suppose we wanted to remove the Income, Illiteracy, and Area variables from the model in Exercise 2. Use the update function to fit this model.
model2 <- update(model, . ~ . -Income -Illiteracy -Area) 
#the '.' means 'same as in original model'
summary(model2)

#Exercise 4: Let’s assume that we have settled on a model that has HS.Grad and Murder as predictors. Fit this model.
model3 <- lm(Life.Exp ~ HS.Grad + Murder, data=state77)
summary(model3)

#Exercise 5: Add an interaction term to the model in Exercise 4 (3 different ways).
model4 <- lm(Life.Exp ~ HS.Grad + Murder + HS.Grad:Murder, data=state77)
summary(model4)


#Exercise 6: For this and the remaining exercises in this set we will use the model from Exercise 4. Obtain 95% confidence intervals for the coefficients of the two predictor variables.
confint(model3, level=0.95)

#Exercise 7: Predict the Life Expectancy for a state where 55% of the population are High School graduates, and the murder rate is 8 per 100,000.
predict(model3,data.frame(HS.Grad=55,Murder=8))

#Exercise 8: Obtain a 98% confidence interval for the mean Life Expectancy in a state where 55% of the population are High School graduates, and the murder rate is 8 per 100,000.
predict(model3,data.frame(HS.Grad=55,Murder=8),interval="confidence",level=0.98)

#Exercise 9: Obtain a 98% confidence interval for the Life Expectancy of a person living in a state where 55% of the population are High School graduates, and the murder rate is 8 per 100,000.
predict(model3,data.frame(HS.Grad=55,Murder=8),interval="prediction",level=0.98)

#Exercise 10: Since our model only has two predictor variables, we can generate a 3D plot of our data and the fitted regression plane. Create this plot.
library(rgl)
plotdat <- expand.grid(HS.Grad=seq(34,70,by=2),Murder=seq(1,16,by=1))
plotdat$pred1 <- predict(model3,newdata=plotdat)
with(state77,plot3d(HS.Grad,Murder,Life.Exp,  col="blue", size=1, type="s"))
with(plotdat,surface3d(unique(HS.Grad),unique(Murder),pred1,
        alpha=0.5,front="line", back="line"))
  
 #Vamos fazer a analise de variancia (ANOVA) da regressao: Calculo das previsoes, erros e erros quadrados  
D <- data.frame( y<- c(2, 5, 7, 9, 10, 13, 20),
 x1<- c(14, 16, 27, 42, 39, 50, 83),
 x2<- c(70, 75, 144, 190, 210, 235, 400))  
 
 
# Multiple Linear Regression Example 
#Calculo do desvio padrao dos erros e dos coeficientes 
fit <- lm(y ~ x1 + x2, data=D)
summary(fit) # show results

#A regressao explica 97% da variabilidade dos dados


confint(fit, level=0.90) # CIs for model parameters 
#Nenhum parametro e significativo

coefficients(fit) # model coefficients
fitted(fit) # predicted values
residuals(fit) # residuals
anova(fit) # anova table 
vcov(fit) # covariance matrix for model parameters 

#Isto significa que a hipotese de que todos parametros sao 0 não pode ser aceita. 


D <- data.frame( y<- c(1, 2, 3, 4),
                 x1<- c(1.3, 2.7, 0.5, 5.0),
                 x2<- c(3.6, 1.8, 0.6, 0.3))  
 
# Multiple Linear Regression Example 
fit <- lm(y ~ x1 + x2, data=D)
summary(fit) # show results
#Nenhum parametro e significativo com confiança de 90% (não é significativo, pois inclui zero). 
